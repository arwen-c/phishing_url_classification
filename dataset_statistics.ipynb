{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T14:17:39.037948Z",
     "start_time": "2023-11-20T14:17:38.968824200Z"
    }
   },
   "outputs": [],
   "source": [
    "#import drill package and add it to the jupyter notebook environment\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T14:25:43.063302700Z",
     "start_time": "2023-11-20T14:25:43.057766200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source of the dataset\n",
    "\n",
    "author = {Feng, Tao and Yue, Chuan},\n",
    "title = {Visualizing and Interpreting RNN Models in URL-Based Phishing Detection},\n",
    "year = {2020},\n",
    "isbn = {9781450375689},\n",
    "publisher = {Association for Computing Machinery},\n",
    "address = {New York, NY, USA},\n",
    "url = {https://doi.org/10.1145/3381991.3395602},\n",
    "doi = {10.1145/3381991.3395602},\n",
    "booktitle = {Proceedings of the 25th ACM Symposium on Access Control Models and Technologies},\n",
    "pages = {13–24},\n",
    "numpages = {12},\n",
    "keywords = {visualization, interpretation, phishing detection, deep learning},\n",
    "location = {Barcelona, Spain},\n",
    "series = {SACMAT '20}\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import into a global dataframe\n",
    "\n",
    "⚠ The data that we import is already encoded into numbers for their RNN analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T14:17:42.220166200Z",
     "start_time": "2023-11-20T14:17:40.698789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shapes:\n",
      "\n",
      "Train set: (1247489, 150)  Validation set: (155936, 150)  Test set: (155937, 150)\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "dill_file=\"vonDataset20180426.dill\"\n",
    "with open(dill_file, 'rb') as f:\n",
    "        pickleData=dill.load( f )\n",
    "        train_x,train_y=pickleData[\"train_x\"],pickleData[\"train_y\"]\n",
    "        val_x,val_y=pickleData[\"val_x\"],pickleData[\"val_y\"]\n",
    "        test_x,test_y=pickleData[\"test_x\"],pickleData[\"test_y\"]\n",
    "        char_to_int=pickleData[\"char_to_int\"]\n",
    "print(\"Feature Shapes:\\n\")\n",
    "print(\"Train set: {}\".format(train_x.shape), \n",
    "      \" Validation set: {}\".format(val_x.shape),\n",
    "      \" Test set: {}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T14:17:42.222162700Z",
     "start_time": "2023-11-20T14:17:42.216661100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'|': 90, 'F': 51, 'd': 16, '!': 78, 'V': 67, '?': 48, '{': 94, 'P': 54, 'E': 49, '�': 88, 'B': 46, 'N': 60, 'a': 4, '5': 28, '8': 25, 'l': 15, ']': 97, 'G': 63, '1': 19, 's': 8, 'f': 26, '9': 33, 'M': 58, '0': 20, '补': 0, 'A': 43, 'L': 55, ' ': 89, ';': 57, 'J': 69, '<': 93, '\\x82': 87, 'e': 3, 'r': 12, 'g': 24, '#': 84, '7': 34, 'H': 65, 'p': 10, 'Y': 68, 'x': 40, 'Ã': 79, '}': 96, 'X': 72, '-': 18, '`': 92, 'S': 52, 'C': 50, 'b': 21, '4': 31, '&': 44, '2': 22, 'i': 9, 'y': 36, 'T': 59, '_': 42, 'Q': 71, '6': 32, '$': 86, '*': 81, '\\x83': 85, '\\n': 30, 'O': 62, '~': 75, 'K': 73, '[': 98, 'D': 41, 'z': 47, 'I': 53, '3': 27, '.': 7, 'Z': 70, 'c': 6, 'o': 5, 'W': 66, '\\x13': 95, ')': 83, 'h': 14, ':': 29, 'w': 17, 'u': 23, '%': 35, 'Â': 80, '未': 99, ',': 76, 'j': 45, 'v': 38, '=': 39, 'k': 37, '@': 77, '+': 74, 'U': 64, '/': 1, 'R': 61, 'q': 56, '(': 82, \"'\": 91, 'm': 13, 't': 2, 'n': 11}\n"
     ]
    }
   ],
   "source": [
    "print(pickleData[\"char_to_int\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T14:17:42.233259100Z",
     "start_time": "2023-11-20T14:17:42.225704600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0 14  2  2 10 29  1  1  4 13  3 15  9 14  4  9 12  7 12 23  1  9\n",
      " 13  4 24  3  8  1 32 28 28 20 20 28 31 22 19 27  1 34 31 28 27 28 27 34\n",
      " 32 33 32 34 33  1]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T14:18:15.590350500Z",
     "start_time": "2023-11-20T14:18:15.569151500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconversion of the data into str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T14:18:25.318933900Z",
     "start_time": "2023-11-20T14:18:25.309895300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "补补补补补补补补补补补补补补补补补补补http://www.topky.sk/gl/409791/1859935/Umrela-mu-babka--zacal-patrat-a-nalez-ho-sokoval--Odhalil-nekale-praktiky-okradania-dochodcov\n"
     ]
    }
   ],
   "source": [
    "#Transform the data back into string using the char_to_int dictionary\n",
    "def int_to_char(x):\n",
    "    res=''.join([list(char_to_int.keys())[list(char_to_int.values()).index(i)] for i in x])\n",
    "    #remove from res the beginning of the string that had zero as a value\n",
    "    # res=res[res.find('h'):]#is every string starting with 'h'?\n",
    "    return res\n",
    "\n",
    "#function that transforms the whole matrix into a list of strings\n",
    "def matrix_to_list(x):\n",
    "    res=[]\n",
    "    for i in range(x.shape[0]):\n",
    "        res.append(int_to_char(x[i]))\n",
    "    return res\n",
    "\n",
    "print(int_to_char(train_x[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tldextract\n",
    "\n",
    "def feature_vector(x: np.ndarray[str]) -> np.ndarray[int]:\n",
    "    \"\"\"\n",
    "    functions that transform the data into a feature vector, inspired by the article PHISH-SAFE URL Features based Phishing Detection System using Machine Learning.pdf\n",
    "    feature 1 : number of dots in the URL\n",
    "    feature 2 : number of hyphens in the URL\n",
    "    feature 3 : number of @ in the URL\n",
    "    feature 4 : length of the URL\n",
    "    feature 5 : number of digits in the URL\n",
    "    feature 6 : number of // in the URL\n",
    "    feature 7 : use of \"HTTPS\"\n",
    "    feature 8 : number of time \"HTTP\" appears\n",
    "    feature 9 : 1 if the IP address is used rather than the domain, 0 otherwise\n",
    "    feature 10 : detection of suspicious word. equals to the number of the suspicious words, the suspicious word being in the list [token, confirm, security, PayPal, login, signin, bank, account, update]\n",
    "    feature 11 : Position of Top-Level Domain: This feature checks the position of top-level domain at proper place in URL.\n",
    "\n",
    "    Have been used in the article but won't be used in our implementation:\n",
    "    Domains count in URL: Phishing URL may contain more than one domain in URL. Two or more domains is used to redirect address.\n",
    "    DNS lookup: If the DNS record is not available then the website is phishing. The life of phishing site is very short, therefore; this DNS information may notbe available after some time.\n",
    "    Inconsistent URL: If the domain name of suspicious web page is not matched  with the WHOIS database record, then the web page is considered as phishing.\n",
    "    Age of Domain: If the age of website is less than 6 month, then chances of fake web page are more.\n",
    "\n",
    "    :param x:\n",
    "    :return a vector of size number of URL * 11 :\n",
    "    \"\"\"\n",
    "    vector = np.array([x.shape[0], 11])\n",
    "    # handcrafted straight forward features\n",
    "    for i, url in enumerate(x):\n",
    "        vector[i][0] = url.count(\".\")\n",
    "        vector[i][1] = url.count(\"-\")\n",
    "        vector[i][2] = url.count(\"@\")\n",
    "        vector[i][3] = len(url)\n",
    "        vector[i][4] = sum(c.isdigit() for c in url)\n",
    "        vector[i][5] = url.count(\"//\")\n",
    "        vector[i][6] = url.count(\"https\")\n",
    "        vector[i][7] = url.count(\"http\")\n",
    "        vector[i][8] = int(tldextract.extract(url).domain == tldextract.extract(url).ipv4)\n",
    "        # suspicious word detection\n",
    "        suspicious_words = [\"token\", \"confirm\", \"security\", \"PayPal\", \"login\", \"signin\", \"bank\", \"account\", \"update\"]\n",
    "        for word in suspicious_words:\n",
    "            vector[i][9] += url.count(word)\n",
    "        # top level domain detection: give the rank of the first character of the top level domain in the url\n",
    "        vector[i][10] = url.find(tldextract.extract(url).suffix)\n",
    "\n",
    "    # word detection and random word detection\n",
    "    for url in x:\n",
    "        tldextract.extract(url)  # https://pypi.org/project/tldextract/\n",
    "    return vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:26:34.598729500Z",
     "start_time": "2023-11-20T14:26:34.442589700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m feature_vector(int_to_char(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mndarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtrain_x\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m))\n",
      "\u001B[1;31mTypeError\u001B[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "feature_vector(int_to_char(np.ndarray([train_x[10]])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:28:28.700120200Z",
     "start_time": "2023-11-20T14:28:28.689389900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleansing ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
